{
    "docs": [
        {
            "location": "/", 
            "text": "Homepage for the project\n\n\nResources\n\n\n[[Source use cases and Project URLS]]\n\n\nVoyage Author Accuracy\n\n\nThis page uses MKDOCS\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Home"
        }, 
        {
            "location": "/#this-page-uses-mkdocs", 
            "text": "For full documentation visit  mkdocs.org .", 
            "title": "This page uses MKDOCS"
        }, 
        {
            "location": "/#commands", 
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/Current-List-of-Israeli-Sources-and-Referring-Sites/", 
            "text": "", 
            "title": "Current List of Israeli Sources and Referring Sites"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/", 
            "text": "Database:\n\n\nNov 5:\n\n\n*add the archiving of source articles that are found, and have this reflected in the interface\n\n\nWeekly sweeps\n\n\nNov 5:\n\n\n\n\nstarted without a hitch\n\n\nneed to add all the necessary referring sites\n\n\n\n\nOct 29:\n\n\n\n\nin a day, found 161, not linear, first hour was a lot, then found less in the next 23\n\n\n\n\nOct 22:\n\n\n\n\nJai/Roger will look into setting up an instance of UTMediaCAT to scan twitter and domains that use Newspaper/RSS \n\n\n\n\nImage preservation and style sheets preservation\n\n\nOct 15\n\n\n\n\ngenerally, the WPull is working really well, takes about 100 seconds to generate the WARC \n\n\noccasionally, some strange issues, like black background, but doesn't affect the text and links\n\n\n\n\nWARC\n\n\nNov 5\n\n\n\n\nWARC'ing is not done: there was a memory crash\n\n\nWARC: dynamic number of processes based on how much memory is available\n\n\nWARCs have been checked\n\n\n\n\nOct 29\n\n\n\n\nWARC'ing also can take up a lot of memory, approx 2 minutes for each hit, need to implement a queue with a maximum number of simultaneous processes.\n\n\n\n\nOct 8\n\n\n\n\nRoger and Jai managed to get the PhantomJS/WPull to get the WARC, but there's a problem with the speed\n\n\nRoger/Jai believe that they can make it faster by figuring out what elements to ignore --- here's what Roger writes: \"we can manually force it to generate files in 2 mins for each url, and the results can be sill good. However, as a result, it is likely that a few images will be missing.( you can check the attachments to see sample file generated by this strategy)\"\n\n\n\n\nOct 1:\n\n\n\n\nRoger figure out how to use PhantomJS \n WPull to get an html output, which runs javascript. This week Jai/Roger will look to see if it's possible to do direct to WARC (preferable to having an export to WARC or conversion from HTML). If so, that's what we'll use.\n\n\n\n\nOLD:\n\n\n\n\nWarC vs Zotero-like ability to snapshot? Jai: snapshot is possible. Kim writes why WARC is far preferable. Problem: not easy to WARC a javascript\n\n\nPhantomJS:  does a PDF of the webpage, but the \"print-out\" version, doesn't have selectable text (not good)\n\n\nWKHTMLtoPDF: renders most of the javascript, linux-version\n\n\nChrome extension works but it is very dependent, one possibility is to have an auto-clicker that runs through Chrome--seems to work best, but would need to have an instance of chromium to handle it\n\n\nSelenium: another project -- checking into it. \n\n\nChrome javascript is available, but it isn't trivial to automate. Question: does the DSU have WARC'ing functionality they can give us? Maybe talk to Anya before going to all the trouble.\n\n\nYuya/Roger also looking at Warc issue\n\n\n\n\nCrawler (Plan B) Test on NYT, CNN, BBC\n\n\nNov 5:\n\n\n\n\ndatabase/save state: not yet working; don't go with sql-lite; use MySQL or other\n\n\nwe will increase server capacity to 40 gb \n 4gb RAM, \n\n\nWe are targetting December to finish implementing the database, and getting ready to do a baseline crawl.\n\n\n\n\nOct 29:\n\n\n\n\ndebugged problem of not picking anything up\n\n\nmemory issue: even with six minute interval of memory logging, we're not able to see why memory usage goes up suddenly\n\n\none possible solution is to do save state/database: probably best to do this for a variety of reasons\n\n\nanother solution is hashing and perhaps 64 bit hashing.\n\n\nanother possible solution: paging: using hard drive for RAM \n\n\nwhether we need more server capacity\n\n\n\n\nOct 22:\n\n\n\n\nnothing picked up yet\n\n\nfreezes, and memory issues: debugging (William), need to catch just before it crashes\n\n\n\n\n\n\nOct 15:\n\n\n\n\nsome problems with the NYT/BBC searches, probably not a unicode problem, and try to see if this is solved by next week.\n\n\n\n\nOct 8\n\n\n\n\nSeems like the unicode problem has been fixed, and the 3 instances have now been running for 12 hours without crashing\n\n\nrates for speed: Cnn is 5000 pages/hour; NYT: 3700 pg/hr; BBC: 1700 pg/hr\n\n\nkeep doing the test, to get a sense; gone through 150,000 pages in last 12 hours with no hits\n\n\n\n\nOct 1\n\n\n\n\nOct 1st: NYT crashed, cnn.com stuck on transcript.cnn.com, BBC going about half the speed of cnn.com. Next week: figure out an average rate per hour or day. \n\n\nOct 1st: three instances running. We'll add exception handler to deal with normal breakdowns so that the crawl continues.\n\n\n\n\nOptimizing plan b crawler:\n\n\nPriority Queuing (Oct 15):\n\n\n\n\nsearch google for all aliases and then queue those hits.\n\n\nproblem: google doesn't search embedded links, would need a source code search for that, so Alejandro is going to write to google.\n\n\nlooking at getting backlinks from a company.\n\n\n\n\nmulti-threading on single site:\n\n\n\n\nwhere one process is downloading/queuing and one process is analyzing metadata \n\n\nleave until later, need to iron out bugs with crawler first.\n\n\n\n\nmulti-threading for multiple sites:\n\n\nOct 15\n\n\n\n\nImplemented multi-threading\n\n\nImplemented log per site\n\n\n\n\nOct 8\n\n\n\n\nWilliam will implement newspaper code for analysis, and Yuya will test it; \n\n\nWilliam/Yuya will also implement a log per web domain per crawl (instead of all together)\n\n\nfixes that were implemented to crawler need to be implemented in the multi-thread crawler, needs to be done manually - William\n\n\nkeep on eye on this: readability issue was implemented over the summer, and this makes the crawl faster, but that also leads to greater instability, the source of the errors we saw this week.\n\n\n\n\nnotes\n\n\n\n\nOct 1: \n\n\nquestion regarding regular expressions, and whether there should be some kind of automated interface to alert user of url patterns or subdomains that are not bringing up hits - we'll think about this.\n\n\nremember what's not an article - Not yet assigned\n\n\na way to discover new articles, eg \"this week's stories\"  - Not yet assigned\n\n\n\n\n3rd party search - Not yet assigned\n\n\n\n\n\n\ndependent on upgrading python, perhaps by Sep 17th, if not Sep 24th\n\n\n\n\nunicode issue that needs to be tested, but it has been incorporated.\n\n\nupgrade is done, \"crawler straying off\" issue - crawler seems to get stuck in video or other path: Yuya will look for ways to produce a \"regular expression\" of URL (white/black list) \n-- Yuya has implemented a filter, so that user can manually insert black list: either normal string or reg ex.\n\n\nproblem that the crawler is treating the same url as distinct urls - William has a solution (Sep 24th) - fixed\n\n\ncode needs to be modified so it doesn't skip something - fixed\n\n\nquestion: does anyone get mad at us, kick out our user agent?\n\n\n\n\nCSS Selectors\n\n\nNov 5\n\n\n\n\nneed some examples from Vinnie of CSS Selectors that need regular expression trouble shooting\n\n\n\n\nOct 29\n\n\n\n\nCSS selector interface needs to be implemented\n\n\ngenerally going well, and will go over with William next week\n\n\nsome problems from last week persist.\n\n\n\n\nOct 22\n\n\n\n\nmondoweiss: need regular expressions to prevent duplicates b/c the comments are distinct\n\n\ntime zone stuff: apparently not difficult to maintain the TZ, but different websites handle this differently\n\n\nalso issue with the Washington Times, show it to us next week.\n\n\n\n\nOct 15\n\n\n\n\nVinnie is getting through this, and next week she'll bring a spreadsheet\n\n\nVinnie will also see if it's possible to keep both date posted and date modified, \n\n\nYuya will add a column to the database for \"date posted\" in cases where this is available\n\n\n\n\nOct 8\n\n\n\n\nVinnie will start looking at the old article database, when available (Yuya setting up), in order to start comparing column entries with timestamp/date and Author and (1) generate a list of problems (like Eldiflor's) and (2) then start to find CSS Selectors.\n\n\n\n\nOld\n\n\n\n\nVinnie \n Alejandro learned how to find css selectors, Vinnie will try to do a few for Sep 24th.\n\n\nVinnie has done most of the sites, and had trouble with two sites\n\n\nin addition to CSS selector: need reg ex to identify the metadata, eg. if author says \"by Name\", need reg ex that can strip out \"by\". So Vinnie will eventually need to write the regex for each site where there's an issue. -- not for now.\n\n\nAlejandro will send a list of sites to add to Vinnie, and Yuya will run an instance that won't be infinite to find which sites are not working for author/date/timestamp pick up - Sep 24\n\n\n\n\nAlias/Tags\n\n\nOct 15\n\n\n\n\nold code is somewhat in conflict, and needs to be pushed with updates - for Oct 22.\n\n\nJai has written code for scope alias and tags and cache, needs to be tested - OCt 1\n\n\n\n\nInterface\n\n\n\n\nRoger is suggesting to look at the following to update the stats: http://ironsummitmedia.github.io/startbootstrap-sb-admin-2/pages/index.html\n\n\n\n\nUpgrade Newspaper/Python\n\n\n\n\nnewspaper: upgrading python and then this will run, and moved to digital ocean -- done \n\n\nhope to have the IP address for the MedCAT -- Yuya emailed.\n\n\n\n\nTeam\n\n\n\n\nAlejandro will email Paul for the possible other developer. -- leave it for now\n\n\n\n\nLocation\n\n\n\n\nwhen DSU takes over MediaCAT web app, then we will look into having a UTSC subdomain.", 
            "title": "Current Tasks and Notes"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#database", 
            "text": "", 
            "title": "Database:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#nov-5", 
            "text": "*add the archiving of source articles that are found, and have this reflected in the interface", 
            "title": "Nov 5:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#weekly-sweeps", 
            "text": "", 
            "title": "Weekly sweeps"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#nov-5_1", 
            "text": "started without a hitch  need to add all the necessary referring sites", 
            "title": "Nov 5:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-29", 
            "text": "in a day, found 161, not linear, first hour was a lot, then found less in the next 23", 
            "title": "Oct 29:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-22", 
            "text": "Jai/Roger will look into setting up an instance of UTMediaCAT to scan twitter and domains that use Newspaper/RSS", 
            "title": "Oct 22:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#image-preservation-and-style-sheets-preservation", 
            "text": "", 
            "title": "Image preservation and style sheets preservation"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-15", 
            "text": "generally, the WPull is working really well, takes about 100 seconds to generate the WARC   occasionally, some strange issues, like black background, but doesn't affect the text and links", 
            "title": "Oct 15"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#warc", 
            "text": "", 
            "title": "WARC"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#nov-5_2", 
            "text": "WARC'ing is not done: there was a memory crash  WARC: dynamic number of processes based on how much memory is available  WARCs have been checked", 
            "title": "Nov 5"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-29_1", 
            "text": "WARC'ing also can take up a lot of memory, approx 2 minutes for each hit, need to implement a queue with a maximum number of simultaneous processes.", 
            "title": "Oct 29"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-8", 
            "text": "Roger and Jai managed to get the PhantomJS/WPull to get the WARC, but there's a problem with the speed  Roger/Jai believe that they can make it faster by figuring out what elements to ignore --- here's what Roger writes: \"we can manually force it to generate files in 2 mins for each url, and the results can be sill good. However, as a result, it is likely that a few images will be missing.( you can check the attachments to see sample file generated by this strategy)\"", 
            "title": "Oct 8"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-1", 
            "text": "Roger figure out how to use PhantomJS   WPull to get an html output, which runs javascript. This week Jai/Roger will look to see if it's possible to do direct to WARC (preferable to having an export to WARC or conversion from HTML). If so, that's what we'll use.", 
            "title": "Oct 1:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#old", 
            "text": "WarC vs Zotero-like ability to snapshot? Jai: snapshot is possible. Kim writes why WARC is far preferable. Problem: not easy to WARC a javascript  PhantomJS:  does a PDF of the webpage, but the \"print-out\" version, doesn't have selectable text (not good)  WKHTMLtoPDF: renders most of the javascript, linux-version  Chrome extension works but it is very dependent, one possibility is to have an auto-clicker that runs through Chrome--seems to work best, but would need to have an instance of chromium to handle it  Selenium: another project -- checking into it.   Chrome javascript is available, but it isn't trivial to automate. Question: does the DSU have WARC'ing functionality they can give us? Maybe talk to Anya before going to all the trouble.  Yuya/Roger also looking at Warc issue", 
            "title": "OLD:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#crawler-plan-b-test-on-nyt-cnn-bbc", 
            "text": "", 
            "title": "Crawler (Plan B) Test on NYT, CNN, BBC"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#nov-5_3", 
            "text": "database/save state: not yet working; don't go with sql-lite; use MySQL or other  we will increase server capacity to 40 gb   4gb RAM,   We are targetting December to finish implementing the database, and getting ready to do a baseline crawl.", 
            "title": "Nov 5:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-29_2", 
            "text": "debugged problem of not picking anything up  memory issue: even with six minute interval of memory logging, we're not able to see why memory usage goes up suddenly  one possible solution is to do save state/database: probably best to do this for a variety of reasons  another solution is hashing and perhaps 64 bit hashing.  another possible solution: paging: using hard drive for RAM   whether we need more server capacity", 
            "title": "Oct 29:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-22_1", 
            "text": "nothing picked up yet  freezes, and memory issues: debugging (William), need to catch just before it crashes", 
            "title": "Oct 22:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-15_1", 
            "text": "some problems with the NYT/BBC searches, probably not a unicode problem, and try to see if this is solved by next week.", 
            "title": "Oct 15:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-8_1", 
            "text": "Seems like the unicode problem has been fixed, and the 3 instances have now been running for 12 hours without crashing  rates for speed: Cnn is 5000 pages/hour; NYT: 3700 pg/hr; BBC: 1700 pg/hr  keep doing the test, to get a sense; gone through 150,000 pages in last 12 hours with no hits", 
            "title": "Oct 8"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-1_1", 
            "text": "Oct 1st: NYT crashed, cnn.com stuck on transcript.cnn.com, BBC going about half the speed of cnn.com. Next week: figure out an average rate per hour or day.   Oct 1st: three instances running. We'll add exception handler to deal with normal breakdowns so that the crawl continues.", 
            "title": "Oct 1"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#optimizing-plan-b-crawler", 
            "text": "", 
            "title": "Optimizing plan b crawler:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#priority-queuing-oct-15", 
            "text": "search google for all aliases and then queue those hits.  problem: google doesn't search embedded links, would need a source code search for that, so Alejandro is going to write to google.  looking at getting backlinks from a company.", 
            "title": "Priority Queuing (Oct 15):"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#multi-threading-on-single-site", 
            "text": "where one process is downloading/queuing and one process is analyzing metadata   leave until later, need to iron out bugs with crawler first.", 
            "title": "multi-threading on single site:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#multi-threading-for-multiple-sites", 
            "text": "Oct 15   Implemented multi-threading  Implemented log per site   Oct 8   William will implement newspaper code for analysis, and Yuya will test it;   William/Yuya will also implement a log per web domain per crawl (instead of all together)  fixes that were implemented to crawler need to be implemented in the multi-thread crawler, needs to be done manually - William  keep on eye on this: readability issue was implemented over the summer, and this makes the crawl faster, but that also leads to greater instability, the source of the errors we saw this week.", 
            "title": "multi-threading for multiple sites:"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#notes", 
            "text": "Oct 1:   question regarding regular expressions, and whether there should be some kind of automated interface to alert user of url patterns or subdomains that are not bringing up hits - we'll think about this.  remember what's not an article - Not yet assigned  a way to discover new articles, eg \"this week's stories\"  - Not yet assigned   3rd party search - Not yet assigned    dependent on upgrading python, perhaps by Sep 17th, if not Sep 24th   unicode issue that needs to be tested, but it has been incorporated.  upgrade is done, \"crawler straying off\" issue - crawler seems to get stuck in video or other path: Yuya will look for ways to produce a \"regular expression\" of URL (white/black list) \n-- Yuya has implemented a filter, so that user can manually insert black list: either normal string or reg ex.  problem that the crawler is treating the same url as distinct urls - William has a solution (Sep 24th) - fixed  code needs to be modified so it doesn't skip something - fixed  question: does anyone get mad at us, kick out our user agent?", 
            "title": "notes"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#css-selectors", 
            "text": "", 
            "title": "CSS Selectors"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#nov-5_4", 
            "text": "need some examples from Vinnie of CSS Selectors that need regular expression trouble shooting", 
            "title": "Nov 5"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-29_3", 
            "text": "CSS selector interface needs to be implemented  generally going well, and will go over with William next week  some problems from last week persist.", 
            "title": "Oct 29"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-22_2", 
            "text": "mondoweiss: need regular expressions to prevent duplicates b/c the comments are distinct  time zone stuff: apparently not difficult to maintain the TZ, but different websites handle this differently  also issue with the Washington Times, show it to us next week.", 
            "title": "Oct 22"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-15_3", 
            "text": "Vinnie is getting through this, and next week she'll bring a spreadsheet  Vinnie will also see if it's possible to keep both date posted and date modified,   Yuya will add a column to the database for \"date posted\" in cases where this is available", 
            "title": "Oct 15"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-8_3", 
            "text": "Vinnie will start looking at the old article database, when available (Yuya setting up), in order to start comparing column entries with timestamp/date and Author and (1) generate a list of problems (like Eldiflor's) and (2) then start to find CSS Selectors.   Old   Vinnie   Alejandro learned how to find css selectors, Vinnie will try to do a few for Sep 24th.  Vinnie has done most of the sites, and had trouble with two sites  in addition to CSS selector: need reg ex to identify the metadata, eg. if author says \"by Name\", need reg ex that can strip out \"by\". So Vinnie will eventually need to write the regex for each site where there's an issue. -- not for now.  Alejandro will send a list of sites to add to Vinnie, and Yuya will run an instance that won't be infinite to find which sites are not working for author/date/timestamp pick up - Sep 24", 
            "title": "Oct 8"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#aliastags", 
            "text": "", 
            "title": "Alias/Tags"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#oct-15_4", 
            "text": "old code is somewhat in conflict, and needs to be pushed with updates - for Oct 22.  Jai has written code for scope alias and tags and cache, needs to be tested - OCt 1", 
            "title": "Oct 15"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#interface", 
            "text": "Roger is suggesting to look at the following to update the stats: http://ironsummitmedia.github.io/startbootstrap-sb-admin-2/pages/index.html", 
            "title": "Interface"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#upgrade-newspaperpython", 
            "text": "newspaper: upgrading python and then this will run, and moved to digital ocean -- done   hope to have the IP address for the MedCAT -- Yuya emailed.", 
            "title": "Upgrade Newspaper/Python"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#team", 
            "text": "Alejandro will email Paul for the possible other developer. -- leave it for now", 
            "title": "Team"
        }, 
        {
            "location": "/Current-Tasks-and-Notes/#location", 
            "text": "when DSU takes over MediaCAT web app, then we will look into having a UTSC subdomain.", 
            "title": "Location"
        }, 
        {
            "location": "/How-to:-CSS-Selector-Training/", 
            "text": "View Source, search for date, and find the css class name that the date belongs to.\n\n\nTo start, copy and paste jquery into the console.\n\n\nTo search \"$(\"___\")\"\n\n\nTo check, press up, add .text()\n\n\nGet as specific selectors as possible.", 
            "title": "How to: CSS Selector Training"
        }, 
        {
            "location": "/Meeting:-Friday,-July-17th-2015/", 
            "text": "Friday Meeting\n\n\nEldiflor can not continue working as a tester but Kim may pick up from where he left off on some of the documentation\n\n\nTwitter explorer just started running yesterday that is why there was no data for tweets.\n\n\nAlejandro likes the metrics columns on the scope.\n\n\nQ. How to know when to use plan A or B crawler?\n\nA. Some documentation in the user interface could be given however we eventually plan to not give the choice to the user and implement internal logic to determine what crawler to use\n\n\nIssue: Plan B crawler needs more work to work faster\nMethod to make it faster:\n- multi-threading\n- potentially prioritize pages that will contain links to \n\n\nWe could also optimize newspaper because it is taking 100% of cpu on mathlab. At least with newspaper issue is not as big of a deal since it finds the right articles immediately by looking at the RSS\n\n\nIssue: Author detection is poor \nPotential Solution: Extend newspaper with Stanford NLP\nPotential Solution: Human intervention to improve detection of elements in articles.  A possible feature that could be good is to implement rules for each referring site that could hone in on elements like author.  To do this the user would have to enter a CSS Selector to identify the pattern for authors on a specific site.\n\n\nRequest: A page for showing the status of the current crawling process. \n\n\nNext meeting Friday 10:30am", 
            "title": "Meeting: Friday, July 17th 2015"
        }, 
        {
            "location": "/Meeting:-Friday,-July-17th-2015/#friday-meeting", 
            "text": "Eldiflor can not continue working as a tester but Kim may pick up from where he left off on some of the documentation  Twitter explorer just started running yesterday that is why there was no data for tweets.  Alejandro likes the metrics columns on the scope.  Q. How to know when to use plan A or B crawler? \nA. Some documentation in the user interface could be given however we eventually plan to not give the choice to the user and implement internal logic to determine what crawler to use  Issue: Plan B crawler needs more work to work faster\nMethod to make it faster:\n- multi-threading\n- potentially prioritize pages that will contain links to   We could also optimize newspaper because it is taking 100% of cpu on mathlab. At least with newspaper issue is not as big of a deal since it finds the right articles immediately by looking at the RSS  Issue: Author detection is poor \nPotential Solution: Extend newspaper with Stanford NLP\nPotential Solution: Human intervention to improve detection of elements in articles.  A possible feature that could be good is to implement rules for each referring site that could hone in on elements like author.  To do this the user would have to enter a CSS Selector to identify the pattern for authors on a specific site.  Request: A page for showing the status of the current crawling process.   Next meeting Friday 10:30am", 
            "title": "Friday Meeting"
        }, 
        {
            "location": "/Meeting:-Friday,-May-22nd-2015/", 
            "text": "Meeting with Alejandro, Eldiflor, and Dev's\n\n\n\n\nWent through user stories\n\n\nReviewed current user stories and added more based on Alejandro's request\n\n\nIsraeli sources should be called Sources\n\n\nBBC, NYT would be Referencing sites\n\n\nTwitter handles are Referencing handles\n\n\nRequest for front page made available to public displaying essential data\n\n\nAlejandro needs to post the list of Israeli news sources\n\n\nDiscussion of storing all articles and citations so that to give the largest sample\n\n\nDiscussion of how to determine if a site is an Israeli news sources without manually entering it as a Source in the list\n\n\nAlejandro wants to get articles by date \n\n\nAlejandro wants a record of breaking stories and changes on those specific articles over time\n\n\nDecided that we should move more quickly through user stories since we were running out of time\n\n\nStarted going through new user stories Alejandro created\n\n\nTOPSY would be good to look into\n\n\nAsked to Alejandro - What is the objective?\n\n\nAnswer (Abbrev.): To understand digital dissemination of Israeli news sources.  How are References using Sources (ie, embedded url, footnote, text only)? How do you measure if a story has gone taken off or \"gone viral\"? Generally how often do References citing Sources? What authors are doing it? Does traffic set the agenda and how is public opinion affected?  These are all things Alejandro would like to explore.\n\n\nNext meeting Thursday May 28, 1pm at UTSG at BA in a undecided room.\n\n\n\n\nDev Meeting\n\n\n\n\nCompared the 3 projects and as a group decided that Voyage will be the project we use as our foundation.  This is mainly due to its use of the Newspaper but additionally since most team members are already familiar with the project.\n\n\n\n\nTo Do\n\n\n\n\nThe data gathering process has been started on Voyage and we will compile some useful data for Thursday.\n\n\nVoyage code should be added to the UTMediaCAT repo before the next meeting to give Paul and William time to read their code and understand the structure.\n\n\nDev's who haven't already need to get an understanding of all the tools used in Voyage.  Newspaper, Django, etc.", 
            "title": "Meeting: Friday, May 22nd 2015"
        }, 
        {
            "location": "/Meeting:-Friday,-May-22nd-2015/#meeting-with-alejandro-eldiflor-and-devs", 
            "text": "Went through user stories  Reviewed current user stories and added more based on Alejandro's request  Israeli sources should be called Sources  BBC, NYT would be Referencing sites  Twitter handles are Referencing handles  Request for front page made available to public displaying essential data  Alejandro needs to post the list of Israeli news sources  Discussion of storing all articles and citations so that to give the largest sample  Discussion of how to determine if a site is an Israeli news sources without manually entering it as a Source in the list  Alejandro wants to get articles by date   Alejandro wants a record of breaking stories and changes on those specific articles over time  Decided that we should move more quickly through user stories since we were running out of time  Started going through new user stories Alejandro created  TOPSY would be good to look into  Asked to Alejandro - What is the objective?  Answer (Abbrev.): To understand digital dissemination of Israeli news sources.  How are References using Sources (ie, embedded url, footnote, text only)? How do you measure if a story has gone taken off or \"gone viral\"? Generally how often do References citing Sources? What authors are doing it? Does traffic set the agenda and how is public opinion affected?  These are all things Alejandro would like to explore.  Next meeting Thursday May 28, 1pm at UTSG at BA in a undecided room.", 
            "title": "Meeting with Alejandro, Eldiflor, and Dev's"
        }, 
        {
            "location": "/Meeting:-Friday,-May-22nd-2015/#dev-meeting", 
            "text": "Compared the 3 projects and as a group decided that Voyage will be the project we use as our foundation.  This is mainly due to its use of the Newspaper but additionally since most team members are already familiar with the project.", 
            "title": "Dev Meeting"
        }, 
        {
            "location": "/Meeting:-Friday,-May-22nd-2015/#to-do", 
            "text": "The data gathering process has been started on Voyage and we will compile some useful data for Thursday.  Voyage code should be added to the UTMediaCAT repo before the next meeting to give Paul and William time to read their code and understand the structure.  Dev's who haven't already need to get an understanding of all the tools used in Voyage.  Newspaper, Django, etc.", 
            "title": "To Do"
        }, 
        {
            "location": "/Meeting:-Friday-July-24-2015/", 
            "text": "The crawler was speed up by 3% due to changes in the way the yaml file was loaded.\nhttps://github.com/UTMediaCAT/Voyage/commit/c86d97d31e1f06b107a4ae5b6397717af8efceac\n\n\n\n\nThe profiler gave all stats per method (C profiler). Conclusion: the parser takes much longer than the parser.\n\n\nThe c profiler gives a python object that you can do with it anything you want, there might be an easier way to do it.\n\n\nQ: Is there things newspaper does that is unnecessary for us? a configuration we can change?\n\n\nA: The settings for newspaper have been configured already for our purposes (example: we dont download images into the crawler).  The remainder of the settings in Newspaper only fine tune small things.\n\n\n\n\nPlan B crawler goes through 5000 links and gets 1 article\n\n\n\n\nControlled experiment \u2013 target a site that is interesting to the client and use crawler A \n B to get stats of what each crawler gets on the same sample.  There is no condition to stop plan B from getting the same data on each site.  There is a page limit on plan B for each site so that it can move to the next site in a reasonable amount of time.\n\n\nPick 3 sites and set up a separate instance to test over a 3 day period.\n\n\nAnother solution is to cashe the crawlers state so it doesn\u2019t spend time on pages it already has visited whenever the process is restarted.\n\n\nThe 5000 number included all types of files on the web and should only include html pages, right now that figure includes all resource types.\n\n\n\n\n\n\nOther than that are we collecting a good set of data for the client. For the most part we have a good amount of tweets and articles for running over 2 months.\n\n\n\n\n\n\nNext step for developers is to run the crawler for a long time on a focused scope (~3 referring sites) to understand the differences between Newspaper and Plan B.\n\n\n\n\n\n\nNext Meeting: Wednesday July 29 on Skype 5:30pm\n\n\nNext Next Meeting: Thursday July 30 downtown 4:00pm", 
            "title": "Meeting: Friday July 24 2015"
        }, 
        {
            "location": "/Meeting:-Thursday,-July-2nd,-2015/", 
            "text": "how to deal with duplicate articles - comparison isn't easy\n\n\nparse url tokenize dashes underscores slashes compare path part\n\n\nlook at duplicates from same domain or across different domains?\n\n\n\n\nsite count\n\n\n\n\n\n\nI would be wise to submit a bug report the Newspaper highlighting the issue we have been facing with their library\n\n\n\n\nWe discussed having a warning system be made for letting the use know when Newspaper found very few articles and that plan B crawler should be ran.\n\n\nWe discussed the issue of how url shortening and query strings affected our crawler.  Since there is no way of differentiating between each url, it is possible for the system to save the same article more than once because of url differences.", 
            "title": "Meeting: Thursday, July 2nd, 2015"
        }, 
        {
            "location": "/Meeting:-Thursday,-May-28-2015/", 
            "text": "Went over what we did in the last week with Anya\n\n\nDiscussed getting Voyage into the UTMediaCAT repo\n\n\nShowed Voyage data with Alejandro\n\n\nWe need to empty the database so that Eldiflor can have a fresh start with the proper input \n\n\nAlejandro requests that warcs be viewable on Voyage\n\n\nAnya suggests that someone to try and break Voyage.  William will do this job.\n\n\npriority #1 will be the user story to create the \"front page\" for guest users\n\n\nHave a user with no insight into the project look at the \"front page\" to ensure our data is comprehensible\n\n\nAnya is working on getting access to different lab spaces for the dev meetings\n\n\nEldiflor's job is to test user stories.  He must take detailed notes on any issues or potential enhancements for the site. He will report this to the devs\n\n\nAlejandro wants to be able to batch insert references and sources.\n\n\nNeed a new name for the project\n\n\n\n\nDev Meeting\n\n\n\n\nNewspaper does not reliably get the authors name. To make it more reliable would be difficult.  This feature will be low priority\n\n\nPut Voyage onto UTMediaCAT repo\n\n\nDiscussed what preliminary tasks need to be done,\n\n\nAdd batch insert for reference and source user entered items\n\n\nRefresh database for Eldiflor to start tinkering\n\n\nChange stat's and visualization data getting functions to use sql queries : Paul\n\n\nImplement a way to cache the stat's and visualization data so that the visual pages load even faster \n\n\nMake a script to backup the database\n\n\nFix specified batch vulnerability\n\n\nWork on \"front page\" : Jai\n\n\ndo testing : Will\n\n\n\n\n\n\nWe request that Alejandro evaluates all the visualization and give detail notes of what he thinks about them \n\n\nDev meeting will now be Thursday 4-8 at St.George and Friday 12pm at UTSC.  Locations TBD", 
            "title": "Meeting: Thursday, May 28 2015"
        }, 
        {
            "location": "/Meeting:-Thursday,-May-28-2015/#dev-meeting", 
            "text": "Newspaper does not reliably get the authors name. To make it more reliable would be difficult.  This feature will be low priority  Put Voyage onto UTMediaCAT repo  Discussed what preliminary tasks need to be done,  Add batch insert for reference and source user entered items  Refresh database for Eldiflor to start tinkering  Change stat's and visualization data getting functions to use sql queries : Paul  Implement a way to cache the stat's and visualization data so that the visual pages load even faster   Make a script to backup the database  Fix specified batch vulnerability  Work on \"front page\" : Jai  do testing : Will    We request that Alejandro evaluates all the visualization and give detail notes of what he thinks about them   Dev meeting will now be Thursday 4-8 at St.George and Friday 12pm at UTSC.  Locations TBD", 
            "title": "Dev Meeting"
        }, 
        {
            "location": "/Meeting:-Thursday,-November-5,-2015/", 
            "text": "save source articles and referring articles, not just referring\nsource article used a lot by a referring site\n\n\n-william to implement regex for getting the wanted part for css selectors\n\n\nInstructions on how to startup and installation requirements\nExample commands\ncomplex optimizations\nfeatures tour - demonstration of features\n\n\n-bbc doesn't keep links more links lik another good site boost keep in\n- pdf creation vs warc creation for views\n- warc just one site not subsites", 
            "title": "Meeting: Thursday, November 5, 2015"
        }, 
        {
            "location": "/Meeting:-Thursday,-October-22,-2015/", 
            "text": "yuya and william multiprocessing code conflicts, merge and debug\n\n\nnew meeting time: 4-6:30 tues\ntesting crawler - freezing, not yet confirmed though no log output\nerrors \n\n\nqueue hits from google searches not yet\ndo merge of multiprocessing function\n\n\ncheck back in two weeks to see how merging has gone and debugging of multiprocessing function\n\n\nqueuing of backlinks - majestic and ahref\nfor any given web domain many urls with backlinks, some which don't exist\ncan narrow those for particular referring sites interested in\ncan get excel spreadsheet with info from screenshot\n\n\ncss selectors\nregex for mondoweiss\n\n\nnto exhaustive css selectors not even the same across theboard\nstill helpful\n\n\nbacklinksand sites\nduplicate links\n\n\nwarc implemented\n\n\nRogers and Jai to set up instance with newspaper running, run RSS feeds per week\n\n\ndb merge shouldn't be an issue unless crawler updates\nscan twitter handles\n\n\nsometimes crawler will crawl original site page and comment page", 
            "title": "Meeting: Thursday, October 22, 2015"
        }, 
        {
            "location": "/Meeting:-Wednesday,-May-13th-2015/", 
            "text": "-meeting every week, the tentative date and time is Fridays at 10am  \n\n\n-find workstudy student candidates to be a tester for the app (Kim)  \n\n\n-set up their app and make it accessible online again (all students)  \n\n\n-make their user stories accessible online or send them via email (all students)  \n\n\n-complete forms and can bring then to Digital Scholarship Unit (located in the library, AC 270) when they're filled (workstudy students)  \n\n\ndiscussion of project management tools:  \n\n\n-the DSU has redmine and an agile plugin  \n\n\n-slack, piazza  \n\n\nnext meetings:\n\n\nmay 28, 1pm, bahen centre St. George", 
            "title": "Meeting: Wednesday, May 13th 2015"
        }, 
        {
            "location": "/Source-use-cases-and-Project-URLS/", 
            "text": "See email for passwords\n\n\nJai, Yuya, Roger:\n\n\nhttp://kujira.ca/admin\n\n\nAll our project details (CRC\u2019s, Plans, etc.) can found at http://jaisughand.com/voyage/\n\n\nPaul:\n\n\nhttp://104.236.42.23/\n\n\nWilliam:\n\n\nhttp://gq3mwt28.cloudapp.net (must be accessed from a utoronto ip)\n\n\nUSE CASES\n\n\nConsolidated use cases can be accessed and edited via this document:\nhttps://docs.google.com/spreadsheets/d/1FROpHbhkee8Pov1XeePDlL0okeFtnqIhLOvdl7u1I5U/edit?usp=sharing\n\n\nSTANFORD NLP\n\n\nhttp://nlp.stanford.edu/\n\n\nNB (Alejandro): The use cases have been prioritized in the first sheet labelled \"priorities\" (original sheet can be found in second sheet). There are some which may already be done, and have asterisks next to the Use Case ID. Others may be duplicates, and have question marks next to the Use Case ID. Finally, there are some things Alejandro didn't understand about the use case, and these also have question marks (and questions).", 
            "title": "Source use cases and Project URLS"
        }, 
        {
            "location": "/Source-use-cases-and-Project-URLS/#use-cases", 
            "text": "Consolidated use cases can be accessed and edited via this document:\nhttps://docs.google.com/spreadsheets/d/1FROpHbhkee8Pov1XeePDlL0okeFtnqIhLOvdl7u1I5U/edit?usp=sharing", 
            "title": "USE CASES"
        }, 
        {
            "location": "/Source-use-cases-and-Project-URLS/#stanford-nlp", 
            "text": "http://nlp.stanford.edu/  NB (Alejandro): The use cases have been prioritized in the first sheet labelled \"priorities\" (original sheet can be found in second sheet). There are some which may already be done, and have asterisks next to the Use Case ID. Others may be duplicates, and have question marks next to the Use Case ID. Finally, there are some things Alejandro didn't understand about the use case, and these also have question marks (and questions).", 
            "title": "STANFORD NLP"
        }
    ]
}