#Database:
## Nov 5:
*add the archiving of source articles that are found, and have this reflected in the interface

# Weekly sweeps
## Nov 5: 
* started without a hitch
* need to add all the necessary referring sites

## Oct 29: 
* in a day, found 161, not linear, first hour was a lot, then found less in the next 23

## Oct 22:
* Jai/Roger will look into setting up an instance of UTMediaCAT to scan twitter and domains that use Newspaper/RSS 
# Image preservation and style sheets preservation

## Oct 15
* generally, the WPull is working really well, takes about 100 seconds to generate the WARC 
* occasionally, some strange issues, like black background, but doesn't affect the text and links

# WARC
##Nov 5
* WARC'ing is not done: there was a memory crash
* WARC: dynamic number of processes based on how much memory is available
* WARCs have been checked

## Oct 29
* WARC'ing also can take up a lot of memory, approx 2 minutes for each hit, need to implement a queue with a maximum number of simultaneous processes.

## Oct 8
* Roger and Jai managed to get the PhantomJS/WPull to get the WARC, but there's a problem with the speed
* Roger/Jai believe that they can make it faster by figuring out what elements to ignore --- here's what Roger writes: "we can manually force it to generate files in 2 mins for each url, and the results can be sill good. However, as a result, it is likely that a few images will be missing.( you can check the attachments to see sample file generated by this strategy)"

## Oct 1: 
* Roger figure out how to use PhantomJS & WPull to get an html output, which runs javascript. This week Jai/Roger will look to see if it's possible to do direct to WARC (preferable to having an export to WARC or conversion from HTML). If so, that's what we'll use.

## OLD:
* WarC vs Zotero-like ability to snapshot? Jai: snapshot is possible. Kim writes why WARC is far preferable. Problem: not easy to WARC a javascript
* PhantomJS:  does a PDF of the webpage, but the "print-out" version, doesn't have selectable text (not good)
* WKHTMLtoPDF: renders most of the javascript, linux-version
* Chrome extension works but it is very dependent, one possibility is to have an auto-clicker that runs through Chrome--seems to work best, but would need to have an instance of chromium to handle it
* Selenium: another project -- checking into it. 
* Chrome javascript is available, but it isn't trivial to automate. Question: does the DSU have WARC'ing functionality they can give us? Maybe talk to Anya before going to all the trouble.
* Yuya/Roger also looking at Warc issue

# Crawler (Plan B) Test on NYT, CNN, BBC  
## Nov 5:
* database/save state: not yet working; don't go with sql-lite; use MySQL or other
* we will increase server capacity to 40 gb & 4gb RAM, 
* We are targetting December to finish implementing the database, and getting ready to do a baseline crawl.

## Oct 29:
* debugged problem of not picking anything up
* memory issue: even with six minute interval of memory logging, we're not able to see why memory usage goes up suddenly
* one possible solution is to do save state/database: probably best to do this for a variety of reasons
* another solution is hashing and perhaps 64 bit hashing.
* another possible solution: paging: using hard drive for RAM 
* whether we need more server capacity
 
## Oct 22:
* nothing picked up yet
* freezes, and memory issues: debugging (William), need to catch just before it crashes
*  

## Oct 15:
* some problems with the NYT/BBC searches, probably not a unicode problem, and try to see if this is solved by next week.

## Oct 8
* Seems like the unicode problem has been fixed, and the 3 instances have now been running for 12 hours without crashing
* rates for speed: Cnn is 5000 pages/hour; NYT: 3700 pg/hr; BBC: 1700 pg/hr
* keep doing the test, to get a sense; gone through 150,000 pages in last 12 hours with no hits

## Oct 1
* Oct 1st: NYT crashed, cnn.com stuck on transcript.cnn.com, BBC going about half the speed of cnn.com. Next week: figure out an average rate per hour or day. 
* Oct 1st: three instances running. We'll add exception handler to deal with normal breakdowns so that the crawl continues.

# Optimizing plan b crawler:
## Priority Queuing (Oct 15):
* search google for all aliases and then queue those hits.
* problem: google doesn't search embedded links, would need a source code search for that, so Alejandro is going to write to google.
* looking at getting backlinks from a company.

## multi-threading on single site: 
* where one process is downloading/queuing and one process is analyzing metadata 
* leave until later, need to iron out bugs with crawler first.

## multi-threading for multiple sites: 
### Oct 15
* Implemented multi-threading
* Implemented log per site

### Oct 8
* William will implement newspaper code for analysis, and Yuya will test it; 
* William/Yuya will also implement a log per web domain per crawl (instead of all together)
* fixes that were implemented to crawler need to be implemented in the multi-thread crawler, needs to be done manually - William
* keep on eye on this: readability issue was implemented over the summer, and this makes the crawl faster, but that also leads to greater instability, the source of the errors we saw this week.

## notes
* Oct 1: 
* question regarding regular expressions, and whether there should be some kind of automated interface to alert user of url patterns or subdomains that are not bringing up hits - we'll think about this.
* remember what's not an article - Not yet assigned
* a way to discover new articles, eg "this week's stories"  - Not yet assigned
* 3rd party search - Not yet assigned
 
* dependent on upgrading python, perhaps by Sep 17th, if not Sep 24th
- unicode issue that needs to be tested, but it has been incorporated.
- upgrade is done, "crawler straying off" issue - crawler seems to get stuck in video or other path: Yuya will look for ways to produce a "regular expression" of URL (white/black list) 
-- Yuya has implemented a filter, so that user can manually insert black list: either normal string or reg ex.
- problem that the crawler is treating the same url as distinct urls - William has a solution (Sep 24th) - fixed
- code needs to be modified so it doesn't skip something - fixed
- question: does anyone get mad at us, kick out our user agent?

# CSS Selectors
##Nov 5
* need some examples from Vinnie of CSS Selectors that need regular expression trouble shooting


## Oct 29
* CSS selector interface needs to be implemented
* generally going well, and will go over with William next week
* some problems from last week persist.

## Oct 22
* mondoweiss: need regular expressions to prevent duplicates b/c the comments are distinct
* time zone stuff: apparently not difficult to maintain the TZ, but different websites handle this differently
* also issue with the Washington Times, show it to us next week.

## Oct 15
* Vinnie is getting through this, and next week she'll bring a spreadsheet
* Vinnie will also see if it's possible to keep both date posted and date modified, 
* Yuya will add a column to the database for "date posted" in cases where this is available

## Oct 8
* Vinnie will start looking at the old article database, when available (Yuya setting up), in order to start comparing column entries with timestamp/date and Author and (1) generate a list of problems (like Eldiflor's) and (2) then start to find CSS Selectors.

### Old
* Vinnie & Alejandro learned how to find css selectors, Vinnie will try to do a few for Sep 24th.
* Vinnie has done most of the sites, and had trouble with two sites
* in addition to CSS selector: need reg ex to identify the metadata, eg. if author says "by Name", need reg ex that can strip out "by". So Vinnie will eventually need to write the regex for each site where there's an issue. -- not for now.
* Alejandro will send a list of sites to add to Vinnie, and Yuya will run an instance that won't be infinite to find which sites are not working for author/date/timestamp pick up - Sep 24

# Alias/Tags
## Oct 15
* old code is somewhat in conflict, and needs to be pushed with updates - for Oct 22.
* Jai has written code for scope alias and tags and cache, needs to be tested - OCt 1

# Interface
* Roger is suggesting to look at the following to update the stats: http://ironsummitmedia.github.io/startbootstrap-sb-admin-2/pages/index.html

# Upgrade Newspaper/Python
* newspaper: upgrading python and then this will run, and moved to digital ocean -- done 
* hope to have the IP address for the MedCAT -- Yuya emailed.

# Team
* Alejandro will email Paul for the possible other developer. -- leave it for now

# Location
* when DSU takes over MediaCAT web app, then we will look into having a UTSC subdomain.